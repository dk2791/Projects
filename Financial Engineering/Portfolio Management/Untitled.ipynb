{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "width, height, depth = 64, 64, 44\n",
    "nlabels = 3\n",
    "num_steps = 5\n",
    "#method = 1\n",
    "\n",
    "folder_root = '../'\n",
    "file_name = 'E13260_run8_3D_wholeBrain_allConds.mat'\n",
    "\n",
    "\n",
    "\n",
    "def model(width, height, depth, nlabels, num_steps, folder_root, file_name, batchsize, learning_rate, method=1):\n",
    "    \"\"\"\n",
    "    Design three methods to train:\n",
    "    method 1: Seperate one run as training set and validation set\n",
    "    method 2: one run as training set, second run as validation set\n",
    "    method 3: Seperate all data as training set and validation set\n",
    "\n",
    "    Argument:\n",
    "        width, height, depth -- the size of one volume\n",
    "        nlabels -- the number of labels(here is 3 types of brain activity: negative, positive and rest)\n",
    "        num_steps -- the number of iterations in training process\n",
    "        method -- type of method\n",
    "        folder_root -- root which stores files\n",
    "        file_name -- can be a name of mat file in method one; a dictionary stored two runs of a person; a root stored all files\n",
    "\n",
    "    Returns\n",
    "        the loss and accuracy was stored in a txt file\n",
    "    \"\"\"\n",
    "    if method == 1:\n",
    "        root = folder_root + file_name\n",
    "        #result_saved_root = folder_root + 'result/' + \"method_\" + str(method) + \"/\" + os.path.basename(root)[0:11] + '.txt'\n",
    "        #hxf 4/28/2018\n",
    "        #result_saved_root = folder_root + 'result/' + \"method_\" + str(method) + \"/\" + os.path.basename(root)[0:11] + \"_lr\" + str(learning_rate) + \"_n\" + str(num_steps) + '.txt'\n",
    "        #dxz 8/9/2018 Use regular expression to extract the object name and run number from the file name, e.g: E14163, run7\n",
    "        result_saved_root = folder_root + 'result/' + \"method_\" + str(method) + \"/\" + re.findall('E[0-9]+',os.path.basename(root))[0]+'_'+re.findall('run[0-9]+',os.path.basename(root))[0] + \"_lr\" + str(learning_rate) + \"_n\" + str(num_steps) + '.txt'\n",
    "        # get data set\n",
    "        train_X, train_y, val_X, val_y, test_X, test_y = data_preprocessing(root)\n",
    "\n",
    "    elif method == 2:\n",
    "        #result_saved_root = folder_root + 'result/' + \"method_\" + str(method) + \"/\" + file_name['train'][0:6] + '.txt'\n",
    "        #hxf 4/28/2018\n",
    "        #result_saved_root = folder_root + 'result/' + \"method_\" + str(method) + \"/\" + file_name['train'][0:6]  + \"_lr\" + str(learning_rate) + \"_n\" + str(num_steps) + '.txt'\n",
    "        #didn't work, TypeError: string indices must be integers\n",
    "\n",
    "        '''\n",
    "        #still didn't work in the MRI unit cluster\n",
    "        #convert from string to dictionary\n",
    "        #refer to https://stackoverflow.com/questions/988228/convert-a-string-representation-of-a-dictionary-to-a-dictionary\n",
    "        import json\n",
    "        #s = \"{'muffin' : 'lolz', 'foo' : 'kitty'}\"\n",
    "        json_acceptable_string = file_name.replace(\"'\", \"\\\"\")\n",
    "        file_name = json.loads(json_acceptable_string)\n",
    "        '''\n",
    "\n",
    "        #goal: use previous run for training the model, new run's data for testing\n",
    "\n",
    "        #option#1: run 7 is used for training, run 8 is used for validation, run 16 is used for testing\n",
    "        # so fixed here!\n",
    "        # file_name = {\"train\": 'E13260_run8_3D_wholeBrain_allConds.mat', \"val\": 'E13260_run9_3D_wholeBrain_allConds.mat'}\n",
    "        file_name = {\"train\": 'E14163_run7_3D_wholeBrain_allConds.mat', \"val\": 'E14163_run8_3D_wholeBrain_allConds.mat', \"test\": 'E14163_run16_3D_wholeBrain_allConds.mat'}\n",
    "\n",
    "        result_saved_root = folder_root + 'result/' + \"method_\" + str(method) + \"/\" + file_name['train'][0:6] + \"_lr\" + str(learning_rate) + \"_n\" + str(num_steps) + '.txt'\n",
    "        train_root = folder_root + file_name['train']\n",
    "        val_root = folder_root + file_name['val']\n",
    "        train_X, train_y = data_preprocessing(train_root, 'True')\n",
    "        val_X, val_y = data_preprocessing(val_root, 'True')\n",
    "        # hxf 4/30/2018\n",
    "        test_root = folder_root + file_name['test']\n",
    "        test_X, test_y = data_preprocessing(test_root, 'True')\n",
    "\n",
    "        # option#2: mixed run 7 & 8 for training & validation, run 16 is used for testing\n",
    "\n",
    "        # option#3: mixed run 7 & 8 for training & validation, use part of run 16 as for updating the model, rest of run 16 is used for testing\n",
    "\n",
    "        print(train_X.shape)\n",
    "        print(train_y.shape)\n",
    "        print(val_X.shape)\n",
    "        print(val_y.shape)\n",
    "\n",
    "    elif method == 3:\n",
    "        #i.e., file_name is useless\n",
    "        #hxf 4/28/2018\n",
    "        result_saved_root = folder_root + 'result/' + \"method_\" + str(method) + \"/\" + 'allSubjects' + \"_lr\" + str(learning_rate) + \"_n\" + str(num_steps) + '.txt'\n",
    "        #result_saved_root = folder_root + 'result/' + \"method_\" + str(method) + \"/\" + 'whole' +'_lr_'+str(learning_rate)+ '.txt'\n",
    "\n",
    "        #hxf 4/28/2018, if input is only one .mat file\n",
    "        #tempFilename, tempFileExt = os.path.splitext(file_name)\n",
    "        #result_saved_root = folder_root + 'result/' + \"method_\" + str(method) + \"/\" + tempFilename +'_lr_'+str(learning_rate)+ '.txt'\n",
    "\n",
    "        # added testing set\n",
    "        train_X = np.empty((0, width, height, depth))\n",
    "        val_X = np.empty((0, width, height, depth))\n",
    "        train_y = np.empty((3, 0))\n",
    "        val_y = np.empty((3, 0))\n",
    "        test_X = np.empty((0, width, height, depth))\n",
    "        test_y = np.empty((3, 0))\n",
    "        for name in os.listdir(folder_root):\n",
    "            if os.path.basename(name)[-3:] == 'mat':\n",
    "                root = folder_root + name\n",
    "                train_X_batch, train_y_batch, val_X_batch, val_y_batch, test_X_batch, test_y_batch = data_preprocessing(\n",
    "                    root)\n",
    "                train_X = np.concatenate((train_X, train_X_batch), axis=0)\n",
    "                val_X = np.concatenate((val_X, val_X_batch), axis=0)\n",
    "                train_y = np.concatenate((train_y, train_y_batch), axis=1)\n",
    "                val_y = np.concatenate((val_y, val_y_batch), axis=1)\n",
    "                test_X = np.concatenate((test_X, test_X_batch), axis=0)\n",
    "                test_y = np.concatenate((test_y, test_y_batch), axis=1)\n",
    "                \n",
    "        assert (train_X.shape[0] == train_y.shape[1])\n",
    "        assert (val_X.shape[0] == val_y.shape[1])\n",
    "        assert (test_X.shape[0] == test_y.shape[1])\n",
    "\n",
    "        '''\n",
    "        train_X = np.empty((0,width,height,depth))\n",
    "        val_X = np.empty((0,width,height,depth))\n",
    "        train_y = np.empty((3,0))\n",
    "        val_y = np.empty((3,0))\n",
    "        for name in os.listdir(folder_root):\n",
    "            if os.path.basename(name)[-3:] == 'mat':\n",
    "                root = folder_root + name\n",
    "                train_X_batch, train_y_batch, val_X_batch, val_y_batch = data_preprocessing(root)\n",
    "                train_X = np.concatenate((train_X,train_X_batch), axis = 0)\n",
    "                val_X = np.concatenate((val_X,val_X_batch), axis = 0)\n",
    "                train_y = np.concatenate((train_y,train_y_batch), axis = 1)\n",
    "                val_y = np.concatenate((val_y,val_y_batch), axis = 1)\n",
    "        assert(train_X.shape[0]==train_y.shape[1])\n",
    "        assert(val_X.shape[0]==val_y.shape[1])\n",
    "        '''\n",
    "\n",
    "    # build tensorflow CNN\n",
    "    sess = tf.InteractiveSession()\n",
    "    # input\n",
    "    x = tf.placeholder(tf.float32, [None,height*width*depth],name = 'x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None,nlabels],name = 'y-input')\n",
    "\n",
    "    x_image = tf.reshape(x, [-1, height, width, depth, 1])\n",
    "\n",
    "    ## First Convolutional Layer\n",
    "    W_conv1 = weight_variable([5, 5, 5, 1, 7],name= 'W_conv1')\n",
    "    b_conv1 = bias_variable([7],name = 'b_conv1')\n",
    "    h_conv1 = tf.nn.relu(conv3d(x_image, W_conv1, [1,1,1], 'VALID') + b_conv1)\n",
    "    print(h_conv1.get_shape)\n",
    "\n",
    "    h_pool1 = max_pool_3D(h_conv1, [4,4,4], [4,4,4], 'VALID')\n",
    "    print(h_pool1.get_shape)\n",
    "\n",
    "    ## Second Convolutional Layer\n",
    "    W_conv2 = weight_variable([5, 5, 5, 7, 17],name= 'W_conv2')\n",
    "    b_conv2 = bias_variable([17],name = 'b_conv2')\n",
    "    h_conv2 = tf.nn.relu(conv3d(h_pool1, W_conv2, [1,1,1], 'VALID') + b_conv2)\n",
    "    print(h_conv2.get_shape)\n",
    "\n",
    "    h_pool2 = max_pool_3D(h_conv2, [4,4,4], [4,4,4], 'VALID')\n",
    "    print(h_pool2.get_shape)\n",
    "\n",
    "    # ## Third Convolutional Layer\n",
    "    # W_conv3 = weight_variable([5, 5, 5, 17, 7],name= 'W_conv3')\n",
    "    # b_conv3 = bias_variable([7],name = 'b_conv3')\n",
    "    # h_conv3 = tf.nn.relu(conv3d(h_pool2, W_conv3, [1,1,1], 'VALID') + b_conv3)\n",
    "    # print(h_conv3.get_shape)\n",
    "\n",
    "    # h_pool3 = max_pool_3D(h_conv3, [4,4,4], [4,4,4], 'VALID')\n",
    "    # print(h_pool3.get_shape)\n",
    "\n",
    "    ## Fully-connected layer\n",
    "    W_fc1 = weight_variable([2*2*1*17,100], name= 'W_fc1')\n",
    "    b_fc1 = bias_variable([100], name= 'b_fc1')\n",
    "\n",
    "    h_pool3_flat = tf.reshape(h_pool2, [-1, 2*2*1*17])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    print(h_fc1_drop.get_shape)\n",
    "\n",
    "    ## Readout Layer\n",
    "    W_fc2 = weight_variable([100, nlabels], name= 'W_out') # [100, 3]\n",
    "    b_fc2 = bias_variable([nlabels], name= 'b_out') # [3]\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    print(y_conv.get_shape)\n",
    "\n",
    "    ## Train and Evaluate the Model\n",
    "    # set up for optimization (optimizer:ADAM)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)  # 1e-4\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #hxf 4/29/2018 preparing for saving the mode\n",
    "    path, filename = os.path.split(result_saved_root) #refer to https://stackoverflow.com/questions/10507298/splitting-path-strings-into-drive-path-and-file-name-parts\n",
    "    tempFilename=os.path.splitext(filename) #refer to https://stackoverflow.com/questions/678236/how-to-get-the-filename-without-the-extension-from-a-path-in-python\n",
    "    sModelPath = path + \"/\" + tempFilename[0] #without extension\n",
    "\n",
    "    # train\n",
    "    for step in range(num_steps):\n",
    "        print(\"step: \"+str(step))\n",
    "        begin, end, batch_x, batch_y = get_data_batch(train_X, train_y, batchsize, width, height, depth)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_step, feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})\n",
    "        #if step % 8 == 0:\n",
    "        if (step == 0) or (step % 8 == 0): #testing\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cross_entropy, accuracy], feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})\n",
    "            #Calculate validation loss and accuracy\n",
    "            #refer to ratio_0.35_lr_0.0015.py at E:\\hxf\\Faculty\\ColumbiaUniversity\\dataprocess\\multiModal_MachineLearning\\download\\Python\\myCode\\DTIQC\\tensorflow\n",
    "            startTime = time.time()\n",
    "            #evaluating the model using validation dataset, hxf 4/29/2018\n",
    "            if method == 3: #because the testing dataset may be too large, so do it batch\n",
    "                val_accuracy = calculateAccuracy(accuracy, y_conv, x, y_, val_X, val_y, width, height,depth, keep_prob, batch=32)\n",
    "            else: #method 1 &2 only 1 or 2 runs data, so it's ok to run it on the whole testing dataset\n",
    "                val_accuracy=sess.run(accuracy, feed_dict={x: val_X.reshape(val_X.shape[0], width*height*depth), y_: val_y.T, keep_prob: 1.0})\n",
    "            #print(\"Testing accuracy:\", val_accuracy)\n",
    "            average_time = (time.time() - startTime) / 1000\n",
    "            '''\n",
    "            with open(result_saved_root, 'a') as f:\n",
    "                print(\"Step \" + str(step) + \", Begin: \" + str(begin) + \", end: \" + str(end) + \", Minibatch Loss= \" +\n",
    "                    \"{:.4f}\".format(loss) + \", Training accuracy= \" + \"{:.3f}\".format(acc),\", Validation accuracy= \" + \"{:.3f}\".format(val_accuracy),\", average validation time = \" + \"{:.5f}\".format(average_time), file=f)\n",
    "            '''\n",
    "            #dxz 8/9/2018 add totalTrainSize\n",
    "            with open(result_saved_root, 'a') as f:\n",
    "                print(\"Step \" + str(step) + \", Begin: \" + str(begin) + \", end: \" + str(end) + ', totalTrainSize= ' + str(train_X.shape[0]) + \", Minibatch Loss= \" +\n",
    "                    \"{:.4f}\".format(loss) + \", Training accuracy= \" + \"{:.3f}\".format(acc),\", Validation accuracy= \" + \"{:.3f}\".format(val_accuracy),\", average validation time = \" + \"{:.5f}\".format(average_time), file=f)\n",
    "\n",
    "            '''\n",
    "            with open(result_saved_root, 'a') as f:\n",
    "                print(\"Step \" + str(step) + \", Begin: \" + str(begin) + \", end: \" + str(end) + \", Minibatch Loss= \" +\n",
    "                    \"{:.4f}\".format(loss) + \", Training accuracy= \" + \"{:.3f}\".format(acc), file=f)\n",
    "            '''\n",
    "\n",
    "            #based  on  https: // www.researchgate.net / post / How_do_I_know_when_to_stop_training_a_neural_network\n",
    "            #see https://www.youtube.com/watch?v=S4ZUwgesjS8 to prevent overfitting.\n",
    "            #You need to divide the data into a training data set and  a validation data set. You train the nnet on the training data set only. The network calculates the errors on the training data set and the validtion data set. Stop training when the validation error is the minimum. This means that the nnet can generalise to unseen data. If you stop training when the training error is minimum then you will have over fitted and the nnet cannot generalise to unseen data. The validation error is usually bigger than the training error in most cases. This process is called cross validation. There are more complicated methods to do cross validation. Cross validation does not apply just to nnets, but is a way of selecting the best model ( which may be a nnet) that produces the best generalisation to unseen data.\n",
    "            # refer to ratio_0.35_lr_0.0015.py at E:\\hxf\\Faculty\\ColumbiaUniversity\\dataprocess\\multiModal_MachineLearning\\download\\Python\\myCode\\DTIQC\\tensorflow\n",
    "            #if acc>=0.75 and val_accuracy>=0.75: #you can set it up as an input threshold\n",
    "            if acc>=0.55 and val_accuracy>=0.55: #for testing purpose, in reality, need a criteria\n",
    "                startTime = time.time()\n",
    "                if method == 3:  # because the testing dataset may be too large, so do it batch\n",
    "                    test_accuracy = calculateAccuracy(accuracy, y_conv, x, y_, test_X, test_y, width, height, depth,keep_prob, batch=32)\n",
    "                else:  # method 1 &2 only 1 or 2 runs data, so it's ok to run it on the whole testing dataset\n",
    "                    test_accuracy = sess.run(accuracy,feed_dict={x: test_X.reshape(test_X.shape[0], width * height * depth),y_: test_y.T, keep_prob: 1.0})\n",
    "                average_time = (time.time() - startTime) / 1000\n",
    "                print(\"Testing accuracy:\", test_accuracy)\n",
    "                with open(result_saved_root, 'a') as f:\n",
    "                    print(\"......Testing result using Step\" + str(step) + \" model......\", file=f)\n",
    "                    print(\"...Testing accuracy:\" + \"{:.3f}\".format(test_accuracy) + \", average testing time = \" + \"{:.5f}\".format(average_time), file=f)\n",
    "                #save the model\n",
    "                try:\n",
    "                    oTrainSaver\n",
    "                except NameError:\n",
    "                    oTrainSaver=tf.train.Saver()\n",
    "                save_path = sModelPath + '_' + str(step)+'.ckpt'\n",
    "                oTrainSaver.save(sess, save_path)  # filename ends with .ckpt\n",
    "                #save_path = model_name+'_'+str(step)\n",
    "                #if not os.path.isabs(save_path):\n",
    "                #\tsave_path = model_saved+save_path\n",
    "                #oTrainSaver.save(sess, save_path)\n",
    "    print(\"Optimization Finished!\")\n",
    "    with open(result_saved_root, 'a') as f:\n",
    "        print(\"Optimization Finished!\", file=f)\n",
    "    #hxf 4/28/2018, save last epoch's model\n",
    "    #refer to https: // www.tensorflow.org / programmers_guide / saved_model\n",
    "    #https://gist.github.com/omimo/5d393ed5b64d2ca0c591e4da04af6009\n",
    "    #https://stackoverflow.com/questions/38000180/save-tensorflow-model-to-file\n",
    "    try:\n",
    "        oTrainSaver\n",
    "    except NameError:\n",
    "        oTrainSaver = tf.train.Saver()\n",
    "    save_path = sModelPath + '_lastEpoch'+'.ckpt'\n",
    "    oTrainSaver.save(sess, save_path)  # filename ends with .ckpt\n",
    "    #oSaveSess = sess\n",
    "    #oTrainSaver.save(oSaveSess, save_path)  # filename ends with .ckpt\n",
    "    print(\"Trained model was saved!\")\n",
    "    with open(result_saved_root, 'a') as f:\n",
    "        print(\"Trained model was saved!\", file=f)\n",
    "\n",
    "    startTime = time.time()\n",
    "    #evaluating the model using validation dataset, hxf 4/29/2018\n",
    "    if method == 3: #because the testing dataset may be too large, so do it batch\n",
    "        test_accuracy = calculateAccuracy(accuracy, y_conv, x, y_, test_X, test_y, width, height,depth, keep_prob, batch=32)\n",
    "    else: #method 1 &2 only 1 or 2 runs data, so it's ok to run it on the whole testing dataset\n",
    "        test_accuracy=sess.run(accuracy, feed_dict={x: test_X.reshape(test_X.shape[0], width*height*depth), y_: test_y.T, keep_prob: 1.0})\n",
    "    average_time = (time.time() - startTime) / 1000\n",
    "    print(\"......Testing accuracy using the last epotch training model:\", test_accuracy)\n",
    "    #save the val_accuracy to a .txt file\n",
    "    with open(result_saved_root, 'a') as f:\n",
    "        print(\".......Testing result using the last epotch training model......\", file=f)\n",
    "        print(\"...Testing accuracy:\" + \"{:.3f}\".format(test_accuracy)+\", average testing time = \" + \"{:.5f}\".format(average_time), file=f)\n",
    "\n",
    "    '''\n",
    "    #original from Sikai\n",
    "    if method == 3:\n",
    "        val_accuracy = calculateAccuracy(accuracy, y_conv, x, y_, val_X, val_y, width, height,depth, keep_prob, batch=32)\n",
    "        print(\"Testing Accuracy:\", val_accuracy)\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: val_X, \n",
    "        y_: val_y, \n",
    "        keep_prob: 1.0}))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/E14163_run7_3D_wholeBrain_allConds.mat\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/E14163_run7_3D_wholeBrain_allConds.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/python3_cooking/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/E14163_run7_3D_wholeBrain_allConds.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5004d8eac88e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m#root = r'D:\\dataprocess\\shared\\Python\\BrainState\\MRIcluster\\data\\rtfMRI\\wholeBrain\\E14163_run7_8_16_3D_wholeBrain_allConds.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;31m#train_X, train_y, val_X, val_y, test_X, test_y = data_preprocessing('/cluster_NAS/scratch1/xiaofu/data/rtfMRI/exclude4s/half/wholeBrain/E14178_run11_3D_wholeBrain_allConds.mat')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-5004d8eac88e>\u001b[0m in \u001b[0;36mdata_preprocessing\u001b[0;34m(root, get_all, get_valset, train_split, val_split)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \"\"\"\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#300, 60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'I_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mraw_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3_cooking/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \"\"\"\n\u001b[1;32m    206\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3_cooking/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[0;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mbyte_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mmjv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3_cooking/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reader needs file name or open file-like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/E14163_run7_3D_wholeBrain_allConds.mat'"
     ]
    }
   ],
   "source": [
    "### Help.py\n",
    "#! /cluster_NAS/scratch1/xiaofu/local/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import random\n",
    "\n",
    "batch_index = 0\n",
    "\n",
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'.\n",
    "    C = tf.constant(C, name = 'C')\n",
    "    \n",
    "    one_hot_matrix = tf.one_hot(labels, depth=C, axis = 0)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "       \n",
    "    return one_hot\n",
    "\n",
    "def cost(logits, labels):\n",
    "    # Create the placeholders for \"logits\" (z) and \"labels\" (y)\n",
    "    y = tf.placeholder(tf.float32, name = 'labels')\n",
    "    \n",
    "    # Use the loss function\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
    "    \n",
    "    # Create a session\n",
    "    sess = tf.Session()\n",
    "    # Run the session\n",
    "    cost = sess.run(cost, feed_dict={z:logits,y:labels})\n",
    "    sess.close()\n",
    "\n",
    "    return cost\n",
    "\n",
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def get_data_batch(data_x, data_y, batch_size, width, height, depth):\n",
    "    global batch_index\n",
    "    max_ = data_y.shape[1]\n",
    "    begin =batch_index\n",
    "    end = batch_index + batch_size\n",
    "    \n",
    "    if end >= max_:\n",
    "        end = max_\n",
    "        batch_index = -batch_size\n",
    "    y_data = data_y[:, begin:end].T\n",
    "    fetch_data_x= data_x[begin:end,:,:,:]\n",
    "        #resized_image = tf.image.resize_images(images=fetch_data_x, size=(width), method=1)\n",
    "        \n",
    "        #image = sess.run(resized_image)  # (256,256)\n",
    "        #x_data = np.append(x_data, np.asarray(image, dtype='float32')) # (image.data, dtype='float32')\n",
    "\n",
    "    batch_index += batch_size\n",
    "    x_data_= fetch_data_x.reshape(end-begin, width*height*depth)\n",
    "    \n",
    "    return begin, end, x_data_, y_data\n",
    "\n",
    "# Tensorflow part, hxf 6/7/2018, added ,trainable=True\n",
    "def weight_variable(shape,name,trainable=True):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)*0.01\n",
    "    return tf.Variable(initial,name = name,trainable=trainable)\n",
    "\n",
    "def bias_variable(shape,name,trainable=True):\n",
    "    initial = tf.constant(0.1,shape = shape)\n",
    "    return tf.Variable(initial, name = name,trainable=trainable)\n",
    "\"\"\"\n",
    "def weight_variable(shape,name):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)*0.01\n",
    "    return tf.Variable(initial,name = name)\n",
    "\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1,shape = shape)\n",
    "    return tf.Variable(initial, name = name)\n",
    "\"\"\"\n",
    "\n",
    "# con layer\n",
    "def conv3d(x,W, strides, paddingtype):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,strides[0],strides[1],strides[2],1], padding = paddingtype)\n",
    "\n",
    "# pool\n",
    "def max_pool_3D(x,poolsize, strides, paddingtype):\n",
    "    return tf.nn.max_pool3d(x,ksize=[1,poolsize[0],poolsize[1],poolsize[2],1],strides=[1,strides[0],strides[1],strides[2],1],padding=paddingtype)\n",
    "\n",
    "\n",
    "def calculateAccuracy(accuracy, y_conv, x, y_, validation_x, validation_y, width, height,depth, keep_prob, batch=100):\n",
    "    \"\"\"\n",
    "    Since each tensor has memory limited, just separate the whole test set into some small parts\n",
    "    \"\"\"\n",
    "    nums = validation_y.shape[1]\n",
    "    predict_list = []\n",
    "    print(nums)\n",
    "    total_accuracy = 0\n",
    "    data_x = validation_x.reshape(nums,width,height,depth)\n",
    "    sess = tf.get_default_session()\n",
    "    cal_list = range(0, nums, batch)\n",
    "    end_ = cal_list[-1]\n",
    "    for i in cal_list[:-1]:\n",
    "        batch_x, batch_y = data_x[i:i+batch,:,:,:], validation_y[:,i:i+batch]\n",
    "        x_data= batch_x.reshape(batch, width*height*depth)\n",
    "        #8/19/2018 dxz   bug fixed   batch_y should do a transpose here\n",
    "        validation_dic_feed = {x: x_data, y_: batch_y.T,keep_prob:1}\n",
    "        #validation_dic_feed = {x: x_data, y_: batch_y,keep_prob:1}\n",
    "        #validation_dic_feed.update(dict((zip(dropout_placehold_store,[1]*len(dropout_placehold_store)))))\n",
    "        accu = sess.run(accuracy, feed_dict =validation_dic_feed)\n",
    "        predict = sess.run(tf.argmax(y_conv,1), feed_dict =validation_dic_feed)\n",
    "        predict_list.extend(predict)\n",
    "\n",
    "        total_accuracy += (accu*batch)\n",
    "\n",
    "    batch_x, batch_y = data_x[end_:,:,:,:], validation_y[:,end_:]\n",
    "    #8/19/2018 dxz  bug fixed\n",
    "    x_data= batch_x.reshape(nums-end_, width*height*depth)\n",
    "    validation_dic_feed = {x: x_data, y_: batch_y.T, keep_prob:1}\n",
    "    #x_data= batch_x.reshape(nums-end_, width*height)\n",
    "    #validation_dic_feed = {x: x_data, y_: batch_y, keep_prob:1}\n",
    "    #validation_dic_feed.update(dict((zip(dropout_placehold_store,[1]*len(dropout_placehold_store)))))\n",
    "    accu = sess.run(accuracy, feed_dict =validation_dic_feed)\n",
    "    predict = sess.run(tf.argmax(y_conv,1), feed_dict =validation_dic_feed)\n",
    "    predict_list.extend(predict)\n",
    "    total_accuracy += (accu*(nums-end_))\n",
    "\n",
    "    return total_accuracy / nums\n",
    "\n",
    "#def data_preprocessing(root, C=3, get_all='False'):\n",
    "#hxf 4/30/2018 added testing set, but still doesn't work for C=2\n",
    "def data_preprocessing(root, get_all='False', get_valset='True',train_split =0.6,val_split=0.2):\n",
    "    \"\"\"\n",
    "    seperate data into train and validation part\n",
    "\n",
    "    Arguments\n",
    "        root -- root of mat file\n",
    "        get_all='True' -- without split data, return all data and label(one-hot) \n",
    "        get_valset='True' -- with split data, return train, test, val data \n",
    "        get_valset='False' -- train_split+val_split \n",
    "        \n",
    "\n",
    "    Returns\n",
    "            train_X, train_y, val_X, val_y #get_valset='False'\n",
    "            train_X, train_y, val_X, val_y, test_X, test_y #for get_valset='True'\n",
    "\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat(root)  #300, 60\n",
    "    raw_data = np.concatenate((data['I'], data['I_test']), axis=1)\n",
    "    raw_label = np.concatenate((data['labels'], data['labels_test']), axis=0)\n",
    "\n",
    "    train_label_list = [element[0] for _,element in enumerate(raw_label.tolist())]\n",
    "    # count the number of different labels\n",
    "    C = len(set(train_label_list))\n",
    "\n",
    "    data_ = np.array([element for _,element in enumerate(raw_data[0,:])])\n",
    "\n",
    "    train_index_var = []\n",
    "    val_index_var = []\n",
    "    test_index_var = []\n",
    "    for NumLabel in range(C):\n",
    "        locals()['label_'+str(NumLabel)] = [index for index,element in enumerate(train_label_list) if element==NumLabel]\n",
    "        locals()['random_permu_'+str(NumLabel)] = np.random.permutation(len(locals()['label_'+str(NumLabel)]))\n",
    "    #label_0 = [index for index,element in enumerate(train_label_list) if element==0]\n",
    "    #label_1 = [index for index,element in enumerate(train_label_list) if element==1]\n",
    "    #label_2 = [index for index,element in enumerate(train_label_list) if element==2]\n",
    "        if get_valset == 'True': #hxf 4/30/2018 added testing set, i.e., 60% traing, 20% validation, 20% testing\n",
    "            locals()['train_index_'+str(NumLabel)] = np.array(locals()['label_'+str(NumLabel)])[locals()['random_permu_'+str(NumLabel)][0:int(np.ceil(len(locals()['label_'+str(NumLabel)]) * train_split))].tolist()]\n",
    "            locals()['val_index_'+str(NumLabel)] = np.array(locals()['label_'+str(NumLabel)])[locals()['random_permu_'+str(NumLabel)][int(np.ceil(len(locals()['label_'+str(NumLabel)]) * train_split)):int(np.ceil(len(locals()['label_'+str(NumLabel)]) * (train_split+val_split)))].tolist()]\n",
    "            val_index_var.append(locals()['val_index_'+str(NumLabel)])\n",
    "        else: #i.e., 80% training 20% testing\n",
    "            locals()['train_index_'+str(NumLabel)] = np.array(locals()['label_'+str(NumLabel)])[locals()['random_permu_'+str(NumLabel)][0:int(np.ceil(len(locals()['label_'+str(NumLabel)]) * (train_split+val_split)))].tolist()]\n",
    "        locals()['test_index_'+str(NumLabel)] = np.array(locals()['label_'+str(NumLabel)])[locals()['random_permu_'+str(NumLabel)][int(np.ceil(len(locals()['label_'+str(NumLabel)]) * (train_split+val_split))):].tolist()]\n",
    "        \n",
    "        '''BUG HERE\n",
    "        train_index_var.append(locals()['label_'+str(NumLabel)])  ####bugs might comes from here\n",
    "        #val_index_var.append(locals()['random_permu_'+str(NumLabel)])\n",
    "        #test_index_var.append(locals()['test_index_'+str(NumLabel)])\n",
    "        '''\n",
    "\n",
    "        train_index_var.append(locals()['train_index_'+str(NumLabel)])  ####xzd 7/2/2018 \n",
    "        test_index_var.append(locals()['test_index_'+str(NumLabel)])\n",
    "        \n",
    "    if get_all == 'True':\n",
    "        random_index = np.random.permutation(len(train_label_list))\n",
    "        data_X = data_[random_index,:,:,:]\n",
    "        data_y = one_hot_matrix(np.array(train_label_list)[random_index], C)\n",
    "\n",
    "        assert(data_X.shape == data_.shape)\n",
    "        assert(data_y.shape[1] == len(train_label_list))\n",
    "\n",
    "        return data_X, data_y\n",
    "\n",
    "    #random_permu_0 = np.random.permutation(len(label_0))\n",
    "    #random_permu_1 = np.random.permutation(len(label_1))\n",
    "    #random_permu_2 = np.random.permutation(len(label_2))\n",
    "\n",
    "    #if get_valset == 'True': #hxf 4/30/2018 added testing set, i.e., 60% traing, 20% validation, 20% testing\n",
    "    #    train_index_0 = np.array(label_0)[random_permu_0[0:int(np.ceil(len(label_0) * 0.6))].tolist()]\n",
    "    #    val_index_0 = np.array(label_0)[random_permu_0[int(np.ceil(len(label_0) * 0.6)):int(np.ceil(len(label_0) * 0.8))].tolist()]\n",
    "    #    test_index_0 = np.array(label_0)[random_permu_0[int(np.ceil(len(label_0) * 0.8)):].tolist()]\n",
    "    #\n",
    "    #    train_index_1 = np.array(label_1)[random_permu_1[0:int(np.ceil(len(label_1) * 0.6))].tolist()]\n",
    "    #    val_index_1 = np.array(label_1)[random_permu_1[int(np.ceil(len(label_1) * 0.6)):int(np.ceil(len(label_1) * 0.8))].tolist()]\n",
    "    #    test_index_1 = np.array(label_1)[random_permu_1[int(np.ceil(len(label_1) * 0.8)):].tolist()]\n",
    "    #\n",
    "    #    train_index_2 = np.array(label_2)[random_permu_2[0:int(np.ceil(len(label_2) * 0.6))].tolist()]\n",
    "    #    val_index_2 = np.array(label_2)[random_permu_2[int(np.ceil(len(label_2) * 0.6)):int(np.ceil(len(label_2) * 0.8))].tolist()]\n",
    "    #    test_index_2 = np.array(label_2)[random_permu_2[int(np.ceil(len(label_2) * 0.8)):].tolist()]\n",
    "    #else: #i.e., 80% training 20% validation\n",
    "    #    train_index_0 = np.array(label_0)[random_permu_0[0:int(np.ceil(len(label_0) * 0.8))].tolist()]\n",
    "    #    val_index_0 = np.array(label_0)[random_permu_0[int(np.ceil(len(label_0) * 0.8)):].tolist()]\n",
    "    #    train_index_1 = np.array(label_1)[random_permu_1[0:int(np.ceil(len(label_1) * 0.8))].tolist()]\n",
    "    #    val_index_1 = np.array(label_1)[random_permu_1[int(np.ceil(len(label_1) * 0.8)):].tolist()]\n",
    "    #    train_index_2 = np.array(label_2)[random_permu_2[0:int(np.ceil(len(label_2) * 0.8))].tolist()]\n",
    "    #    val_index_2 = np.array(label_2)[random_permu_2[int(np.ceil(len(label_2) * 0.8)):].tolist()]\n",
    "\n",
    "    # train index\n",
    "    train_index = np.concatenate(tuple(train_index_var))\n",
    "    random.shuffle(train_index)\n",
    "    # train X\n",
    "    train_X = data_[train_index, :, :, :]\n",
    "    # train y\n",
    "    train_y = one_hot_matrix(np.array(train_label_list)[train_index], C)\n",
    "    # print(train_X.shape[0])\n",
    "    # print(train_y.shape[0])\n",
    "    \n",
    "    #testing index\n",
    "    test_index = np.concatenate(tuple(test_index_var))\n",
    "    #random.shuffle(test_index)\n",
    "    # testing X\n",
    "    test_X = data_[test_index, :, :, :]\n",
    "    # testing y\n",
    "    test_y = one_hot_matrix(np.array(train_label_list)[test_index], C)\n",
    "    # print(test_X.shape[0])\n",
    "    # print(test_X.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    if get_valset == 'True': #hxf 4/30/2018 added testing set, i.e., 60% traing, 20% validation, 20% testing\n",
    "\n",
    "        #validation index\n",
    "        val_index = np.concatenate(tuple(val_index_var))\n",
    "        random.shuffle(val_index)\n",
    "        # validation X\n",
    "        val_X = data_[val_index, :, :, :]\n",
    "        # validation y\n",
    "        val_y = one_hot_matrix(np.array(train_label_list)[val_index], C)\n",
    "        # print(val_X.shape[0])\n",
    "        # print(val_y.shape[0])\n",
    "        assert (test_X.shape[0] == test_y.shape[1])\n",
    "\n",
    "        assert (train_X.shape[0] == train_y.shape[1])\n",
    "        assert (val_X.shape[0] == val_y.shape[1])\n",
    "        assert ((train_X.shape[0] + val_X.shape[0] + test_X.shape[0]) == len(train_label_list))\n",
    "        return train_X, train_y, val_X, val_y, test_X, test_y\n",
    "    else:\n",
    "        assert (train_X.shape[0] == train_y.shape[1])\n",
    "        assert ((train_X.shape[0] + test_X.shape[0]) == len(train_label_list))\n",
    "        return train_X, train_y, test_X, test_y\n",
    "\n",
    "    '''\t\n",
    "    train_index_0 = np.array(label_0)[random_permu_0[0:int(np.ceil(len(label_0)*0.8))].tolist()]\n",
    "    test_index_0 = np.array(label_0)[random_permu_0[int(np.ceil(len(label_0)*0.8)):].tolist()]\n",
    "    train_index_1 = np.array(label_1)[random_permu_1[0:int(np.ceil(len(label_1)*0.8))].tolist()]\n",
    "    test_index_1 = np.array(label_1)[random_permu_1[int(np.ceil(len(label_1)*0.8)):].tolist()]\n",
    "    train_index_2 = np.array(label_2)[random_permu_2[0:int(np.ceil(len(label_2)*0.8))].tolist()]\n",
    "    test_index_2 = np.array(label_2)[random_permu_2[int(np.ceil(len(label_2)*0.8)):].tolist()]\n",
    "\n",
    "    train_index = np.concatenate((train_index_0,train_index_1,train_index_2))\n",
    "    random.shuffle(train_index)\n",
    "\n",
    "    # train X\n",
    "    train_X = data_[train_index,:,:,:]\n",
    "\n",
    "    val_index = np.concatenate((test_index_0,test_index_1,test_index_2))\n",
    "    random.shuffle(val_index)\n",
    "\n",
    "    # validation X\n",
    "    val_X = data_[val_index,:,:]\n",
    "\n",
    "    # train y\n",
    "    train_y = one_hot_matrix(np.array(train_label_list)[train_index], C)\n",
    "\n",
    "    # validation y\n",
    "    val_y = one_hot_matrix(np.array(train_label_list)[val_index], C)\n",
    "    #print(train_X.shape[0])\n",
    "    #print(train_y.shape[0])\n",
    "\n",
    "    assert(train_X.shape[0]==train_y.shape[1])\n",
    "    assert(val_X.shape[0]==val_y.shape[1])\n",
    "    assert((train_X.shape[0]+val_X.shape[0]) == len(train_label_list))\n",
    "\n",
    "    return train_X, train_y, val_X, val_y\n",
    "    '''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #root='D:\\\\dataprocess\\\\shared\\\\Python\\\\BrainState\\\\MRIcluster\\\\data\\\\rtfMRI\\\\wholeBrain\\\\E14163_run7_8_16_3D_wholeBrain_BlankVSNegative.mat'\n",
    "    root = '../data/E14163_run7_3D_wholeBrain_allConds.mat'\n",
    "    #root = r'D:\\dataprocess\\shared\\Python\\BrainState\\MRIcluster\\data\\rtfMRI\\wholeBrain\\E14163_run7_8_16_3D_wholeBrain_allConds.mat'\n",
    "    print(root)\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y = data_preprocessing(root)\n",
    "    #train_X, train_y, val_X, val_y, test_X, test_y = data_preprocessing('/cluster_NAS/scratch1/xiaofu/data/rtfMRI/exclude4s/half/wholeBrain/E14178_run11_3D_wholeBrain_allConds.mat')\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gimdong-geon/python3_cooking/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.get_shape of <tf.Tensor 'Relu_12:0' shape=(?, 60, 60, 40, 7) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'MaxPool3D_8:0' shape=(?, 15, 15, 10, 7) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Relu_13:0' shape=(?, 11, 11, 6, 17) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'MaxPool3D_9:0' shape=(?, 2, 2, 1, 17) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'dropout_4/mul_1:0' shape=(?, 100) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'add_19:0' shape=(?, 3) dtype=float32>>\n",
      "step: 0\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "range object index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-adf2d9122790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# train with method2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfile_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'../data/E14163_run7_3D_wholeBrain_allConds.mat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'../data/E14163_run8_3D_wholeBrain_allConds.mat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'E14163_run16_3D_wholeBrain_allConds.mat'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# train with method3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# file_pair = {\"train\":'E13260_run8_3D_wholeBrain_allConds.mat',\"val\":'E13260_run9_3D_wholeBrain_allConds.mat'}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f3d9bcae7517>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(width, height, depth, nlabels, num_steps, folder_root, file_name, batchsize, learning_rate, method)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;31m#evaluating the model using validation dataset, hxf 4/29/2018\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#because the testing dataset may be too large, so do it batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#method 1 &2 only 1 or 2 runs data, so it's ok to run it on the whole testing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mval_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-5004d8eac88e>\u001b[0m in \u001b[0;36mcalculateAccuracy\u001b[0;34m(accuracy, y_conv, x, y_, validation_x, validation_y, width, height, depth, keep_prob, batch)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mcal_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mend_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcal_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: range object index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # train with method 1\n",
    "    #model(width, height, depth, nlabels, num_steps, folder_root, file_name, batchsize=16, method=1)\n",
    "    # train with method2\n",
    "    file_pair = {\"train\":'../data/E14163_run7_3D_wholeBrain_allConds.mat',\"val\":'../data/E14163_run8_3D_wholeBrain_allConds.mat',\"test\": 'E14163_run16_3D_wholeBrain_allConds.mat'}\n",
    "    model(width, height, depth, nlabels, num_steps, folder_root, file_name, batchsize=16, learning_rate=0.01,method=3)\n",
    "    # train with method3\n",
    "    # file_pair = {\"train\":'E13260_run8_3D_wholeBrain_allConds.mat',\"val\":'E13260_run9_3D_wholeBrain_allConds.mat'}\n",
    "    #model(width, height, depth, nlabels, num_steps, folder_root, file_pair, batchsize=16, method=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python3_cooking)\n",
   "language": "python",
   "name": "python3_cooking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
